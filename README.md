

# Hey Nova ‚Äì Personal AI Assistant (MVP Stage)

## üìñ Overview

**Hey Nova** is a personal AI assistant inspired by J.A.R.V.I.S. and F.R.I.D.A.Y. from *Iron Man*. Unlike Siri or Alexa, Nova is designed for **deep integration, natural conversation, and proactive assistance**.

This project begins with a **MacBook MVP**: an always-listening assistant that responds to the wake word **‚ÄúHey Nova‚Äù**, understands your speech, routes commands to skills (like checking Notion or opening apps), and speaks back naturally using configurable voices.

Future stages will expand to Windows, iPhone, and cross-device operation, with proactive greetings, memory, and encrypted always-on operation.

---

## üéØ MVP Goals

* Respond to **wake word**: ‚ÄúHey Nova‚Äù
* Capture speech, transcribe with **Whisper**
* Route to **skills** (Notion, app control, math, system info)
* Fallback to **LLM (OpenAI API)** for natural conversation
* Speak back using **macOS TTS** (British, American, or custom neural voices)
* Run in **Terminal** as a simple service

---

## üõ†Ô∏è Development Process

### Stage 1: Core Pipeline

1. **Wake Word Detection**

   * Tool: [Picovoice Porcupine](https://picovoice.ai/platform/porcupine/)
   * Detects ‚ÄúHey Nova‚Äù with low CPU usage
   * Alternative: Push-to-talk hotkey

2. **Audio Capture & STT**

   * Library: [faster-whisper](https://github.com/guillaumekln/faster-whisper) (local, efficient Whisper)
   * Captures audio, transcribes speech ‚Üí text

3. **Brain / Router**

   * Router decides if input should go to:

     * **Skill** (e.g., open app, Notion lookup, adjust brightness)
     * **LLM (OpenAI GPT)** for conversation
   * Ensures replies are dynamic, not scripted

4. **TTS (Speech Output)**

   * Default: macOS `AVSpeechSynthesizer` or `say` command (built-in voices)
   * Configurable for British/futuristic tones
   * Future: upgrade to **Azure Neural TTS** / **ElevenLabs**

5. **Skills (Capabilities)**

   * Small Python modules, each with `match()` + `run()`
   * Examples:

     * `app_control.open_app("vscode")`
     * `notion.today_agenda()`
     * `sysinfo.current_time()`
   * Output is **context**, which LLM phrases naturally

---

### Stage 2: Proactive Behavior

* **Event Engine**: triggers on login, Wi-Fi connect, or time windows
* **Context Pack**: aggregates Notion, weather, deadlines, habits
* **LLM** generates greetings dynamically (e.g., ‚ÄúGood evening, Mouhamed. I see you have basketball at 8. Want me to set a reminder?‚Äù)

---

### Stage 3: Multi-Device Expansion

* Extract ‚Äúbrain‚Äù into a **FastAPI/WebSocket server**
* Mac, Windows, iPhone listeners ‚Üí send audio/text to the API
* Each listener only handles **mic + speaker**; brain + skills stay centralized

---

## üß∞ Tools & Technologies

**Core MVP**

* **Language:** Python (or Node.js, but MVP assumes Python)
* **Wake word:** [Porcupine](https://picovoice.ai/)
* **STT:** Whisper (via [faster-whisper](https://github.com/guillaumekln/faster-whisper))
* **Brain:** OpenAI ChatCompletion API (fallback); keyword router for skills
* **TTS:** macOS built-in voices (`AVSpeechSynthesizer` / `say`)
* **Automation:** AppleScript/JXA for app & system control
* **Integration:** Notion API for calendar/tasks

**Future**

* **TTS (Premium):** Azure Neural TTS, Amazon Polly, ElevenLabs
* **LLM (Offline):** LLaMA or GPT4All for private local inference
* **Server:** FastAPI for HTTP/WS API
* **Memory:** SQLite + vector DB for context recall
* **Packaging:** macOS LaunchAgent for autostart; Swift menubar app

---

## üñ•Ô∏è Development Flow

**Run MVP (Night 1):**

1. Clone repo ‚Üí `cd hey-nova`
2. Create virtualenv: `python -m venv .venv && source .venv/bin/activate`
3. Install deps: `pip install faster-whisper openai sounddevice porcupine`
4. Run: `python core/main.py`
5. Speak: ‚ÄúHey Nova‚Äù ‚Üí test with ‚Äúwhat‚Äôs my day?‚Äù or ‚Äúopen VS Code‚Äù

**Structure:**

```
hey-nova/
  core/
    main.py        # Entry point
    config.py      # API keys, voice, preferences
    wakeword/      # Porcupine wrapper
    stt/           # Whisper wrapper
    tts/           # macOS TTS wrapper
    brain/         # Router + persona prompt
    skills/        # Notion, apps, math, sysinfo
  server/          # (later) API for multi-device
  scripts/         # Setup & dev scripts
  README.md
```

---

## üí∏ Cost (MVP vs. Future)

* **Free**: Wake word (Porcupine free tier), Whisper local, macOS TTS, Notion API
* **Optional cloud costs**:

  * OpenAI STT: \$0.006/min (\~\$3/mo @ 15min/day)
  * OpenAI LLM: depends on usage (\~\$5‚Äì\$15/mo light personal use)
  * Premium TTS (Azure/ElevenLabs): \$5‚Äì\$9/mo typical

MVP can run **at \$0** if you keep STT/TTS local. Future upgrades = a few dollars monthly.

---

## üöÄ Roadmap

* **MVP (Week 1‚Äì2)**

  * Wake word, Whisper STT, OpenAI responses, macOS TTS, simple skills

* **Stage 2 (Weeks 3‚Äì4)**

  * Proactive greetings, Notion integration, AppleScript automation

* **Stage 3 (Month 2)**

  * Background service, multi-device support, memory store

* **Stage 4 (Future)**

  * Neural TTS for cinematic voice, cross-device brain, security/encryption

---

## üåå Vision

Hey Nova is not a toy chatbot. It‚Äôs a **personal AI butler** that:

* Greets you first with context, not scripts
* Speaks with a configurable voice/persona
* Controls your apps, reads your agenda, and automates workflows
* Lives across all your devices, but with a single ‚Äúbrain‚Äù
* Grows with you: from MacBook MVP ‚Üí full cross-platform J.A.R.V.I.S.

---

üëâ Would you like me to make this into a **ready-to-use `README.md` file** (with badges, sections, and markdown formatting) so you can drop it straight into your repo?
